{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f649f8d6"
      },
      "source": [
        "# Agentic RAG Implementation - Project Overview\n",
        "\n",
        "The project \"Agentic RAG Implementation\" aims to create a sophisticated Retrieval Augmented Generation (RAG) system by leveraging an agentic framework that incorporates self-reflection, continuous learning, and human-in-the-loop interaction.\n",
        "\n",
        "**Purpose and Main Features:**\n",
        "The primary purpose is to build an intelligent research agent capable of:\n",
        "1.  **Information Retrieval**: Searching for relevant research papers on arXiv, performing general web searches, and extracting content from specified websites or PDF URLs.\n",
        "2.  **Knowledge Management**: Populating and retrieving information from a vector database (`Chroma`) to maintain a persistent knowledge base.\n",
        "3.  **Self-Refinement and Learning**: The agent (`SelfRefineAgent`) can reflect on its performance, identify mistakes, and learn from past experiences stored in `LearningMemory` to improve future task executions.\n",
        "4.  **Human-in-the-Loop**: Incorporating mechanisms for user interaction, such as reviewing and modifying agent plans (`review_after_plan`) and evaluating final answers (`evaluate_final_answer`) to guide the agent's learning process.\n",
        "5.  **Task Orchestration**: A `router_agent` refines initial user queries before delegating them to the `research_agent`.\n",
        "\n",
        "**Problem Solved:**\n",
        "Traditional RAG systems often lack the adaptability and self-correction mechanisms to handle complex, multi-step research tasks or to improve over time. This project addresses these limitations by:\n",
        "*   Enabling the agent to dynamically choose and use appropriate tools based on the task.\n",
        "*   Allowing it to refine queries and content for better database interaction.\n",
        "*   Providing a feedback loop where the agent learns from its successes and failures, reducing repetitive errors and improving the quality of its responses.\n",
        "*   Integrating human oversight to ensure alignment with user intent and ethical considerations.\n",
        "\n",
        "**Technologies Used:**\n",
        "*   **Smolagents**: The core framework for building and orchestrating the intelligent agents.\n",
        "*   **Mistral API**: Utilized for various LLM functionalities, including chat completion, agent creation (`mistral_client.beta.agents.create`), and OCR (`mistral_client.ocr.process`).\n",
        "*   **ChromaDB**: A vector database used for storing and retrieving documents based on semantic similarity. `langchain-chroma` and `MistralAIEmbeddings` are used for integration and embedding generation.\n",
        "*   **Tavily**: For robust web search capabilities (`TavilyWebSearch`) and website content extraction (`ExtractWebsiteContent`).\n",
        "*   **Opik**: An observability tool used for tracing and tracking agent execution and learning processes.\n",
        "*   **Arxiv API**: For programmatically searching and fetching research papers (`FetchingArxivPapers`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "447ff0d2"
      },
      "source": [
        "## Explain the Workflow\n",
        "\n",
        "### End-to-End Workflow of the Agentic RAG System\n",
        "\n",
        "The `main()` function orchestrates the entire agentic RAG system, from initialization to task execution, evaluation, and continuous learning. Below is a detailed breakdown of the workflow:\n",
        "\n",
        "1.  **Initialization and Configuration**: Before any task begins, the `main()` function initializes essential components:\n",
        "    *   **Mistral Client**: `mistral_client` is created to interact with various Mistral AI services (chat, agents, OCR).\n",
        "    *   **ChromaDB**: The `vector_store` (a ChromaDB instance) is set up with `MistralAIEmbeddings` for semantic search and storage.\n",
        "    *   **Learning Memory**: `learning_memory` is instantiated to store and retrieve past experiences for self-refinement.\n",
        "    *   **External Agents**: Specific Mistral AI agents (`mistral_websearch_agent`, `query_refiner_agent`, `db_verifier_agent`) are created for specialized tasks like web search verification, query refinement, and database content verification, respectively.\n",
        "    *   **Tools**: All necessary tools for the `SelfRefineAgent` (e.g., `FetchingArxivPapers`, `OCRAgent`, `TavilyWebSearch`, `ExtractWebsiteContent`, `FillingDatabase`, `RetrieverTool`) are instantiated.\n",
        "    *   **Main Agents**: The `SelfRefineAgent` (named `agent`) and the `router_agent` are instantiated with their respective tools, models, and configurations.\n",
        "\n",
        "2.  **Initial Query and Refinement**:\n",
        "    *   A `user_query` is received (e.g., `task`).\n",
        "    *   The `router_agent` (an instance of `ToolCallingAgent`) is the first point of contact. Its primary role is to refine the user's initial query.\n",
        "    *   It uses the `ReQueryTool`, which in turn leverages the external `query_refiner_agent` (a Mistral AI agent), to make the query more precise and detailed. This step ensures that the subsequent research is focused and effective.\n",
        "\n",
        "3.  **Task Delegation and Learning Enhancement**:\n",
        "    *   Once the query is refined, the `router_agent` delegates it to the `SelfRefineAgent` (named `agent`).\n",
        "    *   Before the `SelfRefineAgent` starts executing, it consults its `LearningMemory`. The `enhance_task_with_learnings` method retrieves relevant past experiences, reflections, and previous answers related to the current task. These learnings are incorporated into the task description, providing the agent with valuable context and suggestions for improvement from prior runs.\n",
        "\n",
        "4.  **Planning and Tool Usage**:\n",
        "    *   The `SelfRefineAgent` then proceeds to plan its actions based on the enhanced task. It determines which tools are most appropriate to address the query.\n",
        "    *   It dynamically calls various tools:\n",
        "        *   `FetchingArxivPapers`: To search for research papers on arXiv.\n",
        "        *   `TavilyWebSearch` and `ExtractWebsiteContent`: For general web searches and extracting content from websites.\n",
        "        *   `OCRAgent`: To process and extract content from PDF URLs.\n",
        "        *   `RetrieverTool`: To retrieve semantically similar documents from the `vector_store`.\n",
        "        *   `FillingDatabase`: To populate the `vector_store` with newly acquired information, ensuring content is chunked and verified by the `db_verifier_agent`.\n",
        "    *   *(Note: While the provided notebook comments out `planning_interval` and `step_callbacks`, if active, a human-in-the-loop review process would occur here. The `review_after_plan` callback would display the agent's plan to the user, allowing them to approve, modify, or cancel it, guiding the agent's strategy before execution.)*\n",
        "\n",
        "5.  **Answer Synthesis**: As the agent gathers information, it synthesizes it to construct a comprehensive final answer to the user's original query.\n",
        "\n",
        "6.  **Final Answer Evaluation and Self-Correction**:\n",
        "    *   After producing a final answer, the `evaluate_final_answer` callback is triggered.\n",
        "    *   This function assesses the quality of the final answer based on criteria like relevancy, depth, structure, and certainty. It might use the `mistral_websearch_agent` to double-check information if needed to gauge certainty.\n",
        "    *   If the evaluation score is satisfactory (e.g., `avg_score > 7.0`), the answer is approved.\n",
        "    *   If the answer is unsatisfactory, feedback is generated and added as a new `TaskStep` to the agent's memory. This feedback prompts the agent to self-correct and refine its answer in a subsequent iteration.\n",
        "\n",
        "7.  **Self-Reflection and Learning**:\n",
        "    *   Regardless of success or failure, the `extract_learnings` method is called at the end of each run.\n",
        "    *   It invokes `generate_reflections` to analyze the agent's performance during the run, identifying `mistakes` and `correct_actions` from the step trace. This process involves a Mistral AI model (`mistral-small-2506`) to parse the run history and provide structured reflections.\n",
        "    *   It also calls `generate_improvement_suggestions` to generate actionable advice for future runs based on the identified mistakes and correct actions.\n",
        "    *   The `reward_output` method interacts with the user to get feedback on the final answer, which contributes to a numerical reward score. This score, along with reflections and suggestions, is then stored as a `LearningExperience` in the `LearningMemory`, ensuring the agent continuously improves over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aea9e9d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install huggingface_hub transformers tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afa6b179"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade opik 'smolagents[telemetry,toolkit]' ddgs arxiv langchain chromadb langchain_chroma mistralai langchain_mistralai firecrawl smolagents openinference-instrumentation-smolagents opentelemetry-sdk opentelemetry-exporter-otlp-proto-http opik requests==2.32.5 transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fq0QsG4B-cc3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import re\n",
        "import arxiv\n",
        "import chromadb\n",
        "from uuid import uuid4\n",
        "import dataclasses\n",
        "from typing import Dict, List, Optional, Any, Callable, Tuple\n",
        "from datetime import datetime\n",
        "from tavily import TavilyClient\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "\n",
        "from smolagents import (\n",
        "    InferenceClientModel,\n",
        "    ToolCallingAgent,\n",
        "    Tool,\n",
        "    DuckDuckGoSearchTool,\n",
        "    TaskStep,\n",
        "    PlanningStep,\n",
        "    ActionStep,\n",
        "    ChatMessage\n",
        ")\n",
        "from mistralai import Mistral\n",
        "from google.colab import userdata\n",
        "from pydantic import BaseModel, BaseConfig\n",
        "from firecrawl import Firecrawl\n",
        "import opik\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1oMu1vLPmaJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class Config(BaseConfig):\n",
        "  HF_INF_TOKEN = \"HF_INF_TOKEN\"\n",
        "  MISTRAL_MODEL = \"mistral-ocr-2505\"\n",
        "  MISTRAL_API_KEY = \"MISTRAL_TEST_KEY\"\n",
        "  OPIK_API_KEY = \"OPIK_API_KEY\"\n",
        "  CHROMA_CLOUD_KEY = \"CHROMA_CLOUD_KEY\"\n",
        "  CHROMA_TENANT = \"CHROMA_TENANT_ID\"\n",
        "  CHROMA_DATABASE = \"AI_Research_Papers\"\n",
        "  TAVILY_API_KEY = \"TAVILY_API_KEY\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Set HF_TOKEN environment variable for smolagents\n",
        "os.environ[\"HF_TOKEN\"] = config.HF_INF_TOKEN\n",
        "\n",
        "# Setup Opik tracing immediately after config is defined\n",
        "os.environ['OPIK_PROJECT_NAME'] = \"smolagents_rag\"\n",
        "opik.configure(api_key=config.OPIK_API_KEY, workspace='Your-Workspace-Name')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_emb = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtUgSqjL1XYI"
      },
      "source": [
        "Step 1: Creating tools for the agent -> Workflow -> Memory + DB -> Tracing + Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDqA9ufRfJT2"
      },
      "outputs": [],
      "source": [
        "# Fetching arxiv papers\n",
        "class FetchingArxivPapers(Tool):\n",
        "    name = \"research_paper_finder\"\n",
        "    description = \"Based on user query, search for the relevant documents/research papers on the arxiv website. The final answer should be in a JSON format. Use this when the query is related to research papers.\"\n",
        "    inputs = {\n",
        "        \"title\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The title the user wants to search for and find related research papers on the arxiv website. This can be multiple keywords to maximize the result.\",\n",
        "        },\n",
        "        \"papers_count\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"The number of papers the search result should return for that title.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"object\"\n",
        "\n",
        "    @opik.track(name=\"retrieving_arxiv_paper\")\n",
        "    def forward(self, title: str, papers_count: int) -> list[dict]:\n",
        "        assert isinstance(title, str), \"Your search query must be a string\"\n",
        "\n",
        "        search_query = f\"'all':{title}\"\n",
        "        search = arxiv.Search(\n",
        "          query=search_query,\n",
        "          max_results=papers_count,\n",
        "          sort_by=arxiv.SortCriterion.SubmittedDate,\n",
        "          sort_order=arxiv.SortOrder.Descending\n",
        "      )\n",
        "\n",
        "        papers = []\n",
        "\n",
        "        client = arxiv.Client()\n",
        "\n",
        "        search = client.results(search)\n",
        "\n",
        "        for result in search:\n",
        "            paper_info = {\n",
        "                    'title': result.title,\n",
        "                    'authors': [author.name for author in result.authors],\n",
        "                    'summary': result.summary,\n",
        "                    'published': result.published,\n",
        "                    'pdf_url': result.pdf_url,\n",
        "                    'arxiv_url': result.entry_id\n",
        "                }\n",
        "            papers.append(paper_info)\n",
        "\n",
        "        return papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGA7327I186A"
      },
      "outputs": [],
      "source": [
        "class FillingDatabase(Tool):\n",
        "    name = \"database_populater\"\n",
        "    description = \"Adding text content (e.g., research papers, blog snippets) to the vector database. Each entry in 'content' should be a substantial, coherent passage that makes sense independently when retrieved, avoiding very short or fragmented sentences. Ensure that the 'metadata_descr' accurately captures the core idea of each content entry to provide sufficient context for retrieval and understanding. Calling this tool must be in JSON format. Only those contents should be added to the DB, which provide some sort of answer to the user query.\"\n",
        "    inputs = {\n",
        "        \"content\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\"type\": \"string\"},\n",
        "            \"description\": \"A list of document contents to populate the VectorDB with. Each item should be a coherent and self-contained passage.\",\n",
        "        },\n",
        "        \"title\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The title of the document that is added to the database in a couple of words. This will be linked to the metadata to provide the origin of the content, for better referencing.\",\n",
        "        },\n",
        "        \"url\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The URL link of the source where the information was retrieved from. Only one link can be used for one action to populate the DB.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, vectordb: Chroma, client: Mistral, db_verifier_id: str, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vectordb = vectordb\n",
        "        self.client = client\n",
        "        self.db_verifier_id = db_verifier_id\n",
        "\n",
        "    @opik.track(name=\"populating_the_DB\")\n",
        "    def forward(self, content: list[str], title: str, url: str) -> None:\n",
        "        assert isinstance(content, list), \"The input must be a list.\"\n",
        "\n",
        "        doc_verified = self.client.beta.conversations.start(\n",
        "            agent_id=self.db_verifier_id,\n",
        "            inputs=json.dumps(content)\n",
        "        )\n",
        "\n",
        "        verified_agent_response = doc_verified.outputs[0].content\n",
        "        print(verified_agent_response)\n",
        "        refined_content = json.loads(verified_agent_response)\n",
        "\n",
        "        documents = [Document(page_content=doc_content, metadata={'description': title, 'url': url}) for doc_content in refined_content]\n",
        "\n",
        "        uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "        self.vectordb.add_documents(documents=documents, ids=uuids)\n",
        "\n",
        "        return \"Successfully added all documents to DB\"\n",
        "\n",
        "class RetrieverTool(Tool):\n",
        "    name = \"data_retriever\"\n",
        "    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query. Calling this tool must be in JSON format.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
        "        },\n",
        "        \"k\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"The number of similar documents to retrieve to answer the user query.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, vectordb: Chroma, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vectordb = vectordb\n",
        "\n",
        "    @opik.track(name=\"retrieving_from_db\")\n",
        "    def forward(self, query: str, k: int) -> str:\n",
        "        assert isinstance(query, str), \"Your search query must be a string\"\n",
        "\n",
        "        docs = self.vectordb.similarity_search_with_score(\n",
        "            query,\n",
        "            k=k,\n",
        "        )\n",
        "\n",
        "        # For ChromaDB Cloud hosting, the smaller the score, the more relevant are the documents to each other\n",
        "        min_score = min([score for _, score in docs])\n",
        "        print(min_score)\n",
        "\n",
        "        if min_score <= 0.3:\n",
        "          return [f\"* {doc.page_content} [{doc.metadata['description']}]\" for doc, score in docs]\n",
        "        else:\n",
        "          return \"No relevant document found, try different tools!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jty7croc2F-1"
      },
      "outputs": [],
      "source": [
        "# Use an OCR Agent as a tool\n",
        "class OCRAgent(Tool):\n",
        "  name = \"ocr_agent\"\n",
        "  description = \"Using an OCR AI model to retrieve the content of PDF files while keeping its structured format. Calling this tool must be in JSON format. Best use case: PDF URLs\"\n",
        "  inputs = {\n",
        "      \"pdf_url\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"The link where the PDF file is located on the web\",\n",
        "      }\n",
        "  }\n",
        "  output_type = \"array\"\n",
        "\n",
        "  def __init__(self, client: Mistral, model: str, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "\n",
        "  @opik.track(name=\"read_with_ocr\")\n",
        "  def forward(self, pdf_url: str) -> list[str]:\n",
        "\n",
        "    pages_content = []\n",
        "\n",
        "    ocr_response = self.client.ocr.process(\n",
        "      model=self.model,\n",
        "      document={\n",
        "          \"type\": \"document_url\",\n",
        "          \"document_url\": pdf_url\n",
        "      },\n",
        "      table_format=\"markdown\",\n",
        "      include_image_base64=False\n",
        "    )\n",
        "\n",
        "    for page in ocr_response.pages:\n",
        "      pages_content.append(page.markdown)\n",
        "\n",
        "    return pages_content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReQueryTool(Tool):\n",
        "  name = \"requery_agent\"\n",
        "  description = \"Refine and make the user query more precise by using an internal agent.\"\n",
        "  inputs = {\n",
        "      \"initial_query\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"The initial query from the user that needs to be refined.\",\n",
        "      }\n",
        "  }\n",
        "  output_type = \"string\"\n",
        "\n",
        "  def __init__(self, client: Mistral, requery_agent_id: str, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.client = client\n",
        "        self.requery_agent_id = requery_agent_id\n",
        "\n",
        "  @opik.track(name=\"improve_user_query\")\n",
        "  def forward(self, initial_query: str) -> str:\n",
        "    response = self.client.beta.conversations.start(\n",
        "        agent_id=self.requery_agent_id,\n",
        "        inputs=initial_query\n",
        "    )\n",
        "    try:\n",
        "      upgraded_query = response.outputs[0].content\n",
        "      return upgraded_query\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return initial_query"
      ],
      "metadata": {
        "id": "iBnLWgrHOH-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgFOVWxPFBoF"
      },
      "outputs": [],
      "source": [
        "class ExtractWebsiteContent(Tool):\n",
        "  name = \"extract_website_content\"\n",
        "  description = \"Using the website URL, it scrapes the website content and returns its content. Best use case: general website urls.\"\n",
        "  inputs = {\n",
        "      \"website_url\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"The link of the website that should be scraped to retrieve its content.\",\n",
        "      }\n",
        "  }\n",
        "  output_type = \"string\"\n",
        "\n",
        "  def __init__(self, client: TavilyClient, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "      self.client = client\n",
        "\n",
        "  @opik.track(name=\"get_site_content\")\n",
        "  def forward(self, website_url: str) -> str:\n",
        "    try:\n",
        "      response = self.client.extract(urls=website_url)\n",
        "\n",
        "      extract_result, url = response['results'][0]['raw_content'], response['results'][0]['url']\n",
        "      return extract_result, url\n",
        "    except Exception as e:\n",
        "      return f'Request failed, following error occured during execution: {e}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TavilyWebSearch(Tool):\n",
        "  name = \"websearch_tool\"\n",
        "  description = \"Using a websearch agent to find information on websites.\"\n",
        "  inputs = {\n",
        "      \"query\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"The query from the user for which the answer is needed.\",\n",
        "      }\n",
        "  }\n",
        "  output_type = \"string\"\n",
        "\n",
        "  def __init__(self, client: TavilyClient, *args, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.client = client\n",
        "\n",
        "  @opik.track(name=\"using_websearch_tool\")\n",
        "  def forward(self, query: str) -> str:\n",
        "    try:\n",
        "      response = self.client.search(query=query, search_depth='fast', max_results=10)\n",
        "\n",
        "      search_result, url = response['results'][0]['content'], response['results'][0]['url']\n",
        "      return search_result, url\n",
        "    except Exception as e:\n",
        "      return f'Request failed, following error occured during execution: {e}'"
      ],
      "metadata": {
        "id": "nRezNGpsmv3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D40tu2t50ok"
      },
      "source": [
        "Step 2: Edit memory to the agent and human in the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qmd2Ez7G2VJe"
      },
      "outputs": [],
      "source": [
        "def display_plan(plan_content):\n",
        "    \"\"\"Display the plan in a formatted way\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"RESEARCH AGENT PLAN\")\n",
        "    print(\"=\" * 60)\n",
        "    print(plan_content)\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "def get_user_choice():\n",
        "    \"\"\"Get user's choice for plan approval\"\"\"\n",
        "    while True:\n",
        "        choice = input(\"\\nChoose an option:\\n1. Approve plan\\n2. Modify plan\\n3. Cancel\\nYour choice (1-3): \").strip()\n",
        "        if choice in [\"1\", \"2\", \"3\"]:\n",
        "            return int(choice)\n",
        "        print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "\n",
        "def get_modified_plan(original_plan):\n",
        "    \"\"\"Allow user to modify the plan\"\"\"\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"MODIFY PLAN\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"Current plan:\")\n",
        "    print(original_plan)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    print(\"What would you like to modify?\")\n",
        "    update_request = input().strip()\n",
        "\n",
        "    modified_plan = mistral_client.chat.complete(\n",
        "        model = \"ministral-8b-2512\",\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Based on user request: {update_request}, update {original_plan} to fit into the needs of the user. Return the updated plan in the same format as the original one.\\\n",
        "                Return only the modified plan and nothing else. What was not modified, should remain the same. Avoid adding comments explaining what was changed.\",\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "    print(modified_plan.choices[0].message.content)\n",
        "    return modified_plan.choices[0].message.content if modified_plan else original_plan\n",
        "\n",
        "@opik.track(name=\"review_after_plan\")\n",
        "def review_after_plan(memory_step, agent):\n",
        "    \"\"\"\n",
        "    Step callback that interrupts the agent after a planning step is created for human review.\n",
        "    This allows for user interaction to review and potentially modify the plan.\n",
        "    \"\"\"\n",
        "    if isinstance(memory_step, PlanningStep):\n",
        "\n",
        "        # Display the created plan\n",
        "        display_plan(memory_step.plan)\n",
        "\n",
        "        # Get user choice\n",
        "        choice = get_user_choice()\n",
        "\n",
        "        if choice == 1:  # Approve plan\n",
        "            print(\"✅ Plan approved! Continuing execution...\")\n",
        "            return\n",
        "\n",
        "        elif choice == 2:  # Modify plan\n",
        "            while choice == 2:\n",
        "              modified_plan = get_modified_plan(memory_step.plan)\n",
        "\n",
        "              display_plan(memory_step.plan)\n",
        "\n",
        "              choice = get_user_choice()\n",
        "\n",
        "            if choice == 3:\n",
        "              agent.interrupt()\n",
        "              return\n",
        "\n",
        "            memory_step.plan = modified_plan\n",
        "\n",
        "            print(\"\\nPlan updated!\")\n",
        "            print(\"✅ Continuing with modified plan...\")\n",
        "            return\n",
        "\n",
        "        elif choice == 3: # Cancel plan\n",
        "            print(\"❌ Execution cancelled by user.\")\n",
        "            agent.interrupt()\n",
        "            return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJkrRvGgK5OI"
      },
      "source": [
        "Step 4: Setting up the Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JQg0I_A0K3Un"
      },
      "outputs": [],
      "source": [
        "# Initiating the database\n",
        "embeddings = MistralAIEmbeddings(\n",
        "    model=\"mistral-embed\",\n",
        "    api_key=config.MISTRAL_API_KEY,\n",
        ")\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"ai_papers\",\n",
        "    embedding_function=embeddings,\n",
        "    chroma_cloud_api_key=config.CHROMA_CLOUD_KEY,\n",
        "    tenant=config.CHROMA_TENANT,\n",
        "    database=config.CHROMA_DATABASE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJfyPUEO_1CV"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "from smolagents.agents import AgentMemory, ToolCallingAgent\n",
        "from pydantic import BaseModel\n",
        "import traceback\n",
        "\n",
        "class AnswerEval(BaseModel):\n",
        "    relevancy: float\n",
        "    depth: float\n",
        "    structure: float\n",
        "    certainty: float\n",
        "    avg_score: float\n",
        "    feedback: str\n",
        "\n",
        "@opik.track(name=\"eval_final_answer\")\n",
        "def evaluate_final_answer(final_answer: str, agent_memory: AgentMemory, agent: ToolCallingAgent, user_query: str) -> bool:\n",
        "  \"\"\"Evaluates the final answer and returns True if satisfactory, False otherwise.\n",
        "  If not satisfactory, adds the feedback to agent's memory for the agent to use.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    while True:\n",
        "      # let the model call tools to investigate what the user wanted: websearch\n",
        "      response = mistral_client.beta.conversations.start(\n",
        "          agent_id=mistral_websearch_agent.id,\n",
        "          inputs=user_query\n",
        "      )\n",
        "      user_query += response.outputs[0].content\n",
        "\n",
        "      chat_response = mistral_client.chat.parse(\n",
        "        model='magistral-small-latest',\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"\n",
        "                  Act as a worldclass evaluator on wide range of topics. Your task is to evaluate the final answer based on the criteria\n",
        "                  outlined below. Each category should receive a float between 0-10, while the avg_score element should be the average score of\n",
        "                  the other categories. After giving out the scores for each category, provide a brief constructive feedback on what should be improved.\n",
        "                  Be very critical since your goal is to only let those answers pass, which can truly give an extensive answer to be used by AI engineers (unless the user specifies they do not want one).\n",
        "                  In your feedback, provide solely what needs to be improved, either in a form of listing or another way.\n",
        "\n",
        "                  Evaluation Criteria:\n",
        "                  - relevancy: how relevant the final answer is to the task\n",
        "                  - depth: to what extent does the answer go into the depth the task requires\n",
        "                  - structure: whether the answer is structured in an easily digestible way to learn the content\n",
        "                  - certainty: how certain you are that the final answer is relevant to the user query. Low score could mean that you do not know for sure what the user really wants/what their query is truly about.\n",
        "                  - avg_score: average score of the scores of all the other categories\n",
        "                  - feedback: your feedback on what to improve. This feedback will be given to the agent for self-correction.\n",
        "                  Your evaluation should be based on the provided Task and Final Answer.\n",
        "                \"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Task: {user_query}; Final Answer: {final_answer}\"\n",
        "            },\n",
        "        ],\n",
        "        response_format=AnswerEval,\n",
        "        temperature=0.2\n",
        "      )\n",
        "\n",
        "      evaluation_result = chat_response.choices[0].message.parsed\n",
        "      if evaluation_result.certainty >= 6.0:\n",
        "        break\n",
        "\n",
        "    if evaluation_result.avg_score > 7.0:\n",
        "      print(f\"✅ Final answer approved with average score: {evaluation_result.avg_score}\")\n",
        "      return True\n",
        "    else:\n",
        "      print(f\"❌ Final answer needs improvement. Average score: {evaluation_result.avg_score}. Feedback: {evaluation_result.feedback}\")\n",
        "      # Add feedback to the agent's memory as a new TaskStep\n",
        "      feedback_step = TaskStep(\n",
        "          task=f\"Previous final answer was unsatisfactory. Feedback: {evaluation_result.feedback}. Average score: {evaluation_result.avg_score}. Please expand and improve your answer based on the feedback. Use tools if needed.\",\n",
        "      )\n",
        "      agent_memory.steps.append(feedback_step)\n",
        "      return False\n",
        "  except Exception as e:\n",
        "    print(f\"Error in evaluate_final_answer: {e}\")\n",
        "    traceback.print_exc()\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement self-refining agent"
      ],
      "metadata": {
        "id": "Cm93IjB7EEWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Any, Callable, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "@dataclass\n",
        "class LearningExperience:\n",
        "    \"\"\"Store learning experiences from agent runs\"\"\"\n",
        "    timestamp: datetime\n",
        "    task: str\n",
        "    correct_actions: List[str]\n",
        "    mistakes: List[str]\n",
        "    final_answer: str\n",
        "    final_answer_reward: float\n",
        "    reflections: str\n",
        "    improvement_suggestions: List[str]\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n"
      ],
      "metadata": {
        "id": "GvUGEnyEh77z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LearningMemory:\n",
        "    \"\"\"Persistent learning memory for self-refinement\"\"\"\n",
        "\n",
        "    def __init__(self, storage_path: str):\n",
        "        self.storage_path = storage_path\n",
        "        self.experiences: List[LearningExperience] = []\n",
        "        self.load_memory()\n",
        "\n",
        "    @opik.track(name=\"load_memory\")\n",
        "    def load_memory(self):\n",
        "      \"\"\"Load the experiences to the memory from the json file\"\"\"\n",
        "      try:\n",
        "        with open(self.storage_path, \"r\") as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "        self.experiences = [\n",
        "            LearningExperience(\n",
        "              timestamp=datetime.fromisoformat(experience['timestamp']),\n",
        "              task=experience['task'],\n",
        "              mistakes=experience['mistakes'],\n",
        "              correct_actions=experience['correct_actions'],\n",
        "              final_answer=experience['final_answer'],\n",
        "              final_answer_reward=experience['final_answer_reward'],\n",
        "              reflections=experience['reflections'],\n",
        "              improvement_suggestions=experience['improvement_suggestions'],\n",
        "              metadata=experience.get('metadata', {})\n",
        "          ) for experience in data\n",
        "            ]\n",
        "\n",
        "      except FileNotFoundError:\n",
        "        print(f\"Learning memory file '{self.storage_path}' not found. Creating an empty one.\")\n",
        "        os.makedirs(os.path.dirname(self.storage_path) or '.', exist_ok=True)\n",
        "        with open(self.storage_path, \"w\") as f:\n",
        "          json.dump([], f)\n",
        "        self.experiences = []\n",
        "      except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from '{self.storage_path}': {e}. Initializing with empty experiences.\")\n",
        "        self.experiences = []\n",
        "      except Exception as e:\n",
        "        print(f\"An unexpected error occurred loading learning memory: {e}. Initializing with empty experiences.\")\n",
        "        self.experiences = []\n",
        "\n",
        "    @opik.track(name=\"save_memory\")\n",
        "    def save_memory(self):\n",
        "      \"\"\"Save the memory to the json file\"\"\"\n",
        "      try:\n",
        "            data = [\n",
        "                {\n",
        "                    'timestamp': experience.timestamp.isoformat(),\n",
        "                    'task': experience.task,\n",
        "                    'mistakes': experience.mistakes,\n",
        "                    'correct_actions': experience.correct_actions,\n",
        "                    'final_answer': experience.final_answer,\n",
        "                    'final_answer_reward': experience.final_answer_reward,\n",
        "                    'reflections': experience.reflections,\n",
        "                    'improvement_suggestions': experience.improvement_suggestions,\n",
        "                    'metadata': experience.metadata\n",
        "                } for experience in self.experiences\n",
        "            ]\n",
        "            os.makedirs(os.path.dirname(self.storage_path) or '.', exist_ok=True)\n",
        "            with open(self.storage_path, 'w') as f:\n",
        "                json.dump(data, f, indent=2)\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving learning memory: {e}\")\n",
        "\n",
        "    @opik.track(name=\"add_experience\")\n",
        "    def add_experience(self, experience: LearningExperience):\n",
        "      \"\"\"Add further experiences to the memory\"\"\"\n",
        "      self.experiences.append(experience)\n",
        "      self.save_memory()\n",
        "\n",
        "    @opik.track(name=\"top_reflections\")\n",
        "    def top_reflections(self, task: str, top_k=3) -> str:\n",
        "      \"\"\"Retrieve the most relevant reflections from past experiences\"\"\"\n",
        "      if not self.experiences:\n",
        "          return \"\", \"\"\n",
        "      task_trimmed = re.search(r'---\\s*Task:\\s*\\n(.*?)\\s*---', task, re.DOTALL)\n",
        "      if task_trimmed:\n",
        "          task_trimmed = task_trimmed.group(1).strip()\n",
        "      else:\n",
        "        task_trimmed = task\n",
        "      task_emb = model_emb.encode(task_trimmed, convert_to_tensor=True)\n",
        "      task_instr = [exp.task for exp in self.experiences]\n",
        "      embs = model_emb.encode(task_instr, convert_to_tensor=True)\n",
        "      scores = util.cos_sim(task_emb, embs)[0]\n",
        "      best = scores.topk(min(top_k, len(self.experiences)))\n",
        "      out_refl = []\n",
        "      out_answer = []\n",
        "      for score, idx in zip(best.values, best.indices):\n",
        "          if score > 0.5:\n",
        "            out_refl.append(f\"- {self.experiences[idx].reflections} (score={score:.2f})\")\n",
        "            out_answer.append(f\"- {self.experiences[idx].final_answer} (score={score:.2f})\")\n",
        "          else:\n",
        "            return \"\"\n",
        "      final_refl = \"\\n\".join(out_refl)\n",
        "      final_output = \"\\n\".join(out_answer)\n",
        "      return final_refl, final_output"
      ],
      "metadata": {
        "id": "BnroAQQemogQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON validator function\n",
        "class ReflectionOutput(BaseModel):\n",
        "    reflection: str\n",
        "    mistakes: List[str]\n",
        "    correct_actions: List[str]"
      ],
      "metadata": {
        "id": "cvYSzh3JiTUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfRefineAgent(ToolCallingAgent):\n",
        "    def __init__(self, *args, learning_memory : LearningMemory, mistral_client: Mistral, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.learning_memory  = learning_memory\n",
        "        self.mistral_client = mistral_client\n",
        "\n",
        "    @opik.track(name=\"enhance_task_with_learnings\")\n",
        "    def enhance_task_with_learnings(self, task: str, relevant_experiences: str, previous_answers: str) -> str:\n",
        "        \"\"\"Enhance the task with insights from past experiences\"\"\"\n",
        "        if not relevant_experiences:\n",
        "          return task\n",
        "        # Add relevant experiences as context to the task\n",
        "        # Another option: instead of adding the result of the previous task to the model as prompt\n",
        "        # it can be stored in the DB and the model retrieves it when needed, so overpopulate the context window.\n",
        "        task_upd = re.search(r'---\\s*Task:\\s*\\n(.*?)\\s*---', task, re.DOTALL).group(1).strip()\n",
        "        if previous_answers:\n",
        "          enhanced_task = f\"{task_upd}\\n\\n--- Here are the feedbacks for improvements from previous similar queries that you should take into account: ---\\n{relevant_experiences}\\n\\n--- Here is the result of previous similar queries, use it if needed as a starting point: ---\\n{previous_answers}\"\n",
        "        else:\n",
        "          enhanced_task = f\"{task_upd}\\n\\n--- Here are the feedbacks for improvements from previous similar queries that you should take into account: ---\\n{relevant_experiences}\"\n",
        "        return enhanced_task\n",
        "\n",
        "    @opik.track(name=\"extract_learnings\")\n",
        "    def extract_learnings(self, task: str, result: str):\n",
        "        \"\"\"Extract learnings from this run and store them\"\"\"\n",
        "\n",
        "        refl_calls = self.generate_reflections(task)\n",
        "\n",
        "        mistakes = refl_calls.mistakes\n",
        "        correct_actions = refl_calls.correct_actions\n",
        "        final_answer = result\n",
        "        final_answer_reward = self.reward_output(final_answer)\n",
        "        final_answer_reward = list(final_answer_reward.values())\n",
        "        reflections = refl_calls.reflection\n",
        "        improvement_suggestions = self.generate_improvement_suggestions(task, correct_actions, mistakes)\n",
        "\n",
        "        def extract_query(query: str) -> str:\n",
        "          \"\"\"Get the improved query from the instructions of the manager agent\"\"\"\n",
        "          task_upd = re.search(r'---\\s*Task:\\s*\\n(.*?)\\s*---', query, re.DOTALL)\n",
        "          if task_upd:\n",
        "              return task_upd.group(1).strip()\n",
        "          return task\n",
        "\n",
        "        task_upd = extract_query(task)\n",
        "\n",
        "        experience = LearningExperience(\n",
        "            timestamp=datetime.now(),\n",
        "            task=task_upd,\n",
        "            correct_actions=correct_actions,\n",
        "            mistakes=mistakes,\n",
        "            final_answer=str(result),\n",
        "            final_answer_reward=final_answer_reward,\n",
        "            reflections=reflections,\n",
        "            improvement_suggestions=improvement_suggestions,\n",
        "            metadata={\n",
        "                'duration': (datetime.now() - self.current_run_start_time).total_seconds(),\n",
        "                'total_steps': len(self.memory.steps)\n",
        "            }\n",
        "        )\n",
        "        self.learning_memory.add_experience(experience)\n",
        "\n",
        "    @opik.track(name=\"generate_reflections\")\n",
        "    def generate_reflections(self, task: str) -> ReflectionOutput:\n",
        "        \"\"\"Generate reflections about this run\"\"\"\n",
        "        task_trimmed = re.search(r'---\\s*Task:\\s*\\n(.*?)\\s*---', task, re.DOTALL)\n",
        "        if task_trimmed:\n",
        "            task_trimmed = task_trimmed.group(1).strip()\n",
        "        else:\n",
        "          task_trimmed = task\n",
        "        steps = self.memory.get_succinct_steps()\n",
        "        history_steps = \"\\n\".join([f\"{s['role']}: {s['content']}\" for s in steps if hasattr(s, 'role')])\n",
        "        prompt = f\"\"\"\n",
        "          Act as an expert at introspectively analysing content. Given the task and the step trace below,\n",
        "          write a concise reflection (<=3 bullet points) that will help a future agent solve\n",
        "          similar tasks better. Make sure to also point out mistakes the agent made in that step.\n",
        "          The mistakes should be in bullet points and should be clear actionable steps.\n",
        "          E.g.: mistake if the agent called the tools in a less effective order -> what to add:\n",
        "          Retrieve any relevant info from the DB before doing websearch or other actions.\n",
        "          Besides finding the mistakes, the correct actions, what the agent did right, should be reinforced in agent,\n",
        "          so make sure to list them too.\n",
        "\n",
        "          Task: {task_trimmed}\n",
        "\n",
        "          Trace:\n",
        "          {history_steps}\n",
        "\n",
        "          Answer in JSON object with reflection, mistakes and correct_actions as the key.\n",
        "          Return ONLY the JSON object and nothing else.\n",
        "        \"\"\"\n",
        "\n",
        "        reflection_response = self.mistral_client.chat.parse(\n",
        "            model='mistral-small-2506',\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            response_format=ReflectionOutput,\n",
        "        )\n",
        "        return reflection_response.choices[0].message.parsed\n",
        "\n",
        "    @opik.track(name=\"generate_improvement_suggestions\")\n",
        "    def generate_improvement_suggestions(self, task: str, correct_actions: List[str], mistakes: List[str]) -> List[str]:\n",
        "        \"\"\"Generate specific improvement suggestions\"\"\"\n",
        "        suggestions = []\n",
        "        task_trimmed = re.search(r'---\\s*Task:\\s*\\n(.*?)\\s*---', task, re.DOTALL)\n",
        "        if task_trimmed:\n",
        "            task_trimmed = task_trimmed.group(1).strip()\n",
        "        else:\n",
        "          task_trimmed = task\n",
        "        prompt = f\"\"\"\n",
        "          Act as an expert to providing clear actions on how to improve the completion of the user query on the topic of {task_trimmed},\n",
        "          provide a list of suggestions for improvements in a form of clear actions that should be taken\n",
        "          to ensure better future execution.\n",
        "          Return only the list of suggestions!\n",
        "          The agent, in the next run, should be able to work with those suggestions as if it was a sole researcher,\n",
        "          who has no access to experts or any group of people. The aim of your suggestions is to improve the research result or answer to the user query.\n",
        "\n",
        "          Bad suggestion: 'Update the information regularly to reflect any changes or updates in the model architecture'.\n",
        "          Since the agent will not be able to do this, since the process lasts until it provided the final answer to the user query.\n",
        "\n",
        "          Mistakes made during the task: {mistakes}\n",
        "          Correct actions to make during the task: {correct_actions}\n",
        "        \"\"\"\n",
        "        suggestion = self.model([ChatMessage(role= \"user\", content= prompt)])\n",
        "        suggestions.append(suggestion.content)\n",
        "\n",
        "        return suggestions\n",
        "\n",
        "    @opik.track(name=\"reward_output\")\n",
        "    def reward_output(self, final_answer: str) -> float:\n",
        "      \"\"\"Nudge the model into the user preferred output\"\"\"\n",
        "\n",
        "      user_feedback = input(\"Did you get the desired response? (Y/N)\").strip()\n",
        "      if user_feedback.upper() == \"Y\":\n",
        "        prompt_features = f\"\"\"\n",
        "          The user liked the following final answer: '{final_answer}'.\n",
        "          Analyze the answer and extract its key desirable features. Return these features as a JSON string.\n",
        "          Example: {{\\\"clarity\\\": \\\"excellent\\\", \\\"conciseness\\\": \\\"good\\\", \\\"detail_level\\\": \\\"high\\\"}}\n",
        "          Feel free to add more criteria than the ones in the example.\n",
        "          \"\"\"\n",
        "\n",
        "      elif user_feedback.upper() == \"N\":\n",
        "        prompt_features = f\"\"\"\n",
        "          The user did NOT like the following final answer: '{final_answer}'.\n",
        "          Analyze the answer and identify its key undesirable features. Return these features as a JSON string.\n",
        "          Example: {{\\\"clarity\\\": \\\"poor\\\", \\\"conciseness\\\": \\\"too verbose\\\", \\\"detail_level\\\": \\\"low\\\"}}\n",
        "          Feel free to add more criteria than the ones in the example.\n",
        "          \"\"\"\n",
        "\n",
        "      else:\n",
        "        print(\"Unrecognised response, defaulting reward to 0.\")\n",
        "        return 0.0\n",
        "\n",
        "      review = self.model([ChatMessage(role= \"user\", content= prompt_features)])\n",
        "      review_content = review.content\n",
        "\n",
        "      prompt_score = f\"\"\"\n",
        "        Score the analysis of the result based on the user's judgment of it: {review_content}.\n",
        "        Return ONLY a float score between 0.0-10.0 and nothing else! Avoid explaining the reason for it or adding anything else!\n",
        "        0.0 indicating that the final result was completely terrible and irrelevant to the user.\n",
        "        10.0 indicating that the final result could completely satisfy the needs of the user.\n",
        "      \"\"\"\n",
        "      result_reward_msg = self.model([ChatMessage(role= \"user\", content= prompt_score)])\n",
        "      try:\n",
        "          result_reward = float(result_reward_msg.content.strip())\n",
        "      except ValueError:\n",
        "          print(f\"Warning: Could not parse reward score from model output: {result_reward_msg.content}\")\n",
        "          result_reward = 0.0 # Default to 0 if parsing fails\n",
        "\n",
        "      return {\"The reward score of your final answer is\": result_reward}\n",
        "\n",
        "    def run(self, task: str, *args, **kwargs):\n",
        "        self.current_run_start_time = datetime.now()\n",
        "        relevant_experiences, previous_answers = self.learning_memory.top_reflections(task)\n",
        "        enhanced_task = self.enhance_task_with_learnings(task, relevant_experiences, previous_answers)\n",
        "        try:\n",
        "            # Run the enhanced task\n",
        "            result = super().run(enhanced_task, *args, **kwargs)\n",
        "\n",
        "            # Analyze the run and extract learnings\n",
        "            self.extract_learnings(task, result)\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            # Extract learnings from failure\n",
        "            self.extract_learnings(task, str(e))\n",
        "            raise"
      ],
      "metadata": {
        "id": "cyvy9M43f1pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_client = Mistral(api_key=config.MISTRAL_API_KEY)\n",
        "# Create mistral agents to chip in during execution\n",
        "# This agent is primarily useful when the user gives a link and asks about its content\n",
        "mistral_websearch_agent = mistral_client.beta.agents.create(\n",
        "    model=\"mistral-small-2506\",\n",
        "    description=\"Agent able to search information over the web to double check the responses of the agent.\",\n",
        "    name=\"Websearch Agent\",\n",
        "    instructions=\"\"\"\n",
        "      Using your `web_search` tool, double check the work of the agent based on the provided `user_query` and `final_answer`.\n",
        "      First, make sure to understand what the user wants, make assumptions if needed, then find out the final answer is missing something that could be relevant to the user.\n",
        "      Return an improved and updated `user_query` that would clarify the user's intention and aim of what they want to know.\n",
        "    \"\"\",\n",
        "    tools=[{\"type\": \"web_search\"}],\n",
        ")\n",
        "\n",
        "query_refiner_agent = mistral_client.beta.agents.create(\n",
        "    model=\"mistral-small-2506\",\n",
        "    description=\"Agent designed to improve and make user queries more precise after performing a web search to understand user intent.\",\n",
        "    name=\"Query Refiner Agent\",\n",
        "    instructions=\"\"\"\n",
        "      Act as an expert at refining and improving existing queries, your goal is to upgrade the user's initial query,\n",
        "      so it can be used by another agent to conduct deeper research on this topic.\n",
        "      If needed, use the `web_search` tool to understand the context and what the user might be looking for.\n",
        "      Based on the search results and your understanding, strictly only rewrite the original `user_query` to be more precise, detailed, and clear.\n",
        "      Return *only* the refined query as a string, without any additional conversational text.\n",
        "      Vital: avoid answering the query!\n",
        "\n",
        "      Example:\n",
        "      initial_query: Teach me what this paper is about: {url of Qwen3-TTS Technical Report }\n",
        "      steps taken: go to the url -> find out that the user is interested about Qwen3-TTS -> refine the query, so the next agent can give a more extensive answer\n",
        "      refined_query (output): What is Qwen3-TTS Technical Report about?\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "db_verifier_agent = mistral_client.beta.agents.create(\n",
        "    model=\"mistral-small-latest\",\n",
        "    name=\"db-verifier-agent\",\n",
        "    description=\"Agent to improve and restructure the provided content into coherent and independently meaningful chunks, optimizing them for retrieval and storage in a vector database.\",\n",
        "    instructions=\"\"\"\n",
        "      You are an expert content optimizer and chunker for a vector database. Your primary role is to take incoming text content, provided as a JSON string representing a list of strings, and process it into discrete, self-contained, and highly retrievable chunks.\n",
        "\n",
        "      Carefully process the provided `content` based on these criteria:\n",
        "      1.  **Coherence and Independence:** Each output chunk must be a coherent and meaningful passage that can stand alone without needing additional context. Avoid very short, fragmented sentences, or content that relies heavily on its neighbors for understanding.\n",
        "      2.  **Optimal for Retrieval:** Restructure the content to maximize its utility when retrieved. This means each chunk should ideally convey a single, complete idea or a closely related set of ideas.\n",
        "      3.  **Structured Output:** Your output must be a list of strings, where each string is an optimized content chunk. The number of output chunks does not need to match the number of input items; you can combine or split content as necessary to meet the above criteria.\n",
        "\n",
        "      Your output should *only* be the JSON representation of the list of strings. Do not include any conversational text, explanations, or other formatting outside the JSON array.\n",
        "      Give no intro at the beginning of your answer, like 'Here is ...', only return the updated text.\n",
        "\n",
        "      Note: if necessary, use your \"web_search_premium\" tool investigate the inputs further to make sure you store independent chunks.\n",
        "\n",
        "      Example:\n",
        "      Input: ['This is a partial sentence. It needs more context to make sense. Here is another idea. It is about AI.', 'And this is a third idea. It is very detailed.']\n",
        "      Output: ['This is a partial sentence. It needs more context to make sense.', 'Here is another idea about AI.', 'This is a third idea, which is very detailed.']\n",
        "    \"\"\",\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Zy6TWDvSQa-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"\"\"What are the most popular scaling laws in LLM training, make sure to save contents to DB\"\"\""
      ],
      "metadata": {
        "id": "I9GG-0DIU-Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H743ZtiW6CBL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "learning_memory = LearningMemory(\"research_agent_memory.json\")\n",
        "@opik.track(name=\"main_run\")\n",
        "def main():\n",
        "    \"\"\"Run the complete plan customization example\"\"\"\n",
        "    print(\"Starting the Agentic RAG Process\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create agent with planning enabled and step callback\n",
        "    # Fill in the final eval function with the user query\n",
        "    evaluate_final_answer.__name__ = \"eval_final_answer\"\n",
        "    eval_final_answer = functools.partial(evaluate_final_answer, user_query=task)\n",
        "\n",
        "    # Initiating the agent\n",
        "    model = InferenceClientModel(\n",
        "        model_id=\"MiniMaxAI/MiniMax-M2\",\n",
        "        provider='auto',\n",
        "        token=config.HF_INF_TOKEN,\n",
        "      )\n",
        "\n",
        "    agent = SelfRefineAgent(\n",
        "        learning_memory = learning_memory,\n",
        "        tools=[\n",
        "            FetchingArxivPapers(),\n",
        "            OCRAgent(client=mistral_client, model=config.MISTRAL_MODEL),\n",
        "            TavilyWebSearch(client=TavilyClient(api_key=config.TAVILY_API_KEY)),\n",
        "            ExtractWebsiteContent(client=TavilyClient(api_key=config.TAVILY_API_KEY)),\n",
        "            FillingDatabase(vectordb=vector_store, client=mistral_client, db_verifier_id=db_verifier_agent.id),\n",
        "            RetrieverTool(vectordb=vector_store),\n",
        "          ],\n",
        "        model=model,\n",
        "        mistral_client=mistral_client,\n",
        "        final_answer_checks=[eval_final_answer],\n",
        "        verbosity_level=2,\n",
        "        planning_interval=6,\n",
        "        step_callbacks={PlanningStep: review_after_plan},\n",
        "        name=\"research_agent\",\n",
        "        description=\"Agent that can conduct research to find research papers, retrieve content from the web and has access to the vector db.\"\n",
        "      )\n",
        "\n",
        "    router_agent = ToolCallingAgent(\n",
        "        tools=[ReQueryTool(client=mistral_client, requery_agent_id=query_refiner_agent.id)],\n",
        "        model=model,\n",
        "        instructions=\"Your sole purpose is to refine the user's query using the `requery_agent` tool and then, without exception, immediately **delegate** the refined query to your managed agent. You **must not** attempt to answer the user's original task yourself.\",\n",
        "        managed_agents=[agent],\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n📋 Task: {task}\")\n",
        "        print(\"\\n🤖 Agent starting execution...\")\n",
        "\n",
        "        result = router_agent.run(task)\n",
        "\n",
        "        print(\"\\n✅ Task completed successfully!\")\n",
        "        print(\"\\n📄 Final Result:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        if \"interrupted\" in str(e).lower():\n",
        "            print(\"\\n🛑 Agent execution was cancelled by user.\")\n",
        "\n",
        "            print(f\"\\n📚 Current memory contains {len(agent.memory.steps)} steps:\")\n",
        "            for i, step in enumerate(agent.memory.steps):\n",
        "                step_type = type(step).__name__\n",
        "                print(f\"  {i + 1}. {step_type}\")\n",
        "\n",
        "            resume_choice = input(\"\\nWould you like to see resume demonstration? (y/n): \").strip().lower()\n",
        "            if resume_choice == \"y\":\n",
        "                print(\"\\n🔄 Resuming execution...\")\n",
        "                try:\n",
        "                    agent.run(task, reset=False)\n",
        "                    print(\"\\n✅ Task completed after resume!\")\n",
        "                    print(\"\\n📄 Final Result:\")\n",
        "                    print(\"-\" * 40)\n",
        "                except Exception as resume_error:\n",
        "                    print(f\"\\n❌ Error during resume: {resume_error}\")\n",
        "                else:\n",
        "                    print(f\"\\n❌ An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNoOdvIZqXde"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Learnings:\")\n",
        "for i, experience in enumerate(learning_memory.experiences):\n",
        "    print(f\"--- Learning Experience {i+1} ---\")\n",
        "    for field_name, field_value in dataclasses.asdict(experience).items():\n",
        "        # Handle lists and other potentially long outputs for better formatting\n",
        "        if isinstance(field_value, list):\n",
        "            print(f\"  {field_name}:\")\n",
        "            for item in field_value:\n",
        "                print(f\"    - {item}\")\n",
        "        else:\n",
        "            print(f\"  {field_name}: {field_value}\")\n",
        "    print(\"-----------------------------------\\n\")"
      ],
      "metadata": {
        "id": "8gB7361NWk-x",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FDXrsQez8H5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nW3QyhG5RFiXy4y2OT_STDyOrcG8MtEn",
      "authorship_tag": "ABX9TyNY5ubRE0zmO+exYRub8sfP"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}