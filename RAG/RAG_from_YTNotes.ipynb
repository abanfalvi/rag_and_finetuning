{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1vEkQfdI7v-9ezg02yJDQknOwbDtDD1zN",
      "authorship_tag": "ABX9TyPXrZTsGrh+5paDbCavmUIj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19ae9ad"
      },
      "source": [
        "# RAG from YT Notes\n",
        "\n",
        "## Goal\n",
        "The primary goal of this project is to create a Retrieval-Augmented Generation (RAG) system. This system will utilize notes generated from YouTube video transcripts as its data source for retrieval, enabling effective question answering.\n",
        "\n",
        "## Process\n",
        "The RAG system follows a multi-stage process:\n",
        "1. **Tokenization/Embedding**: Transcripts are first tokenized and embedded to convert text into numerical representations.\n",
        "2. **Chunking**: The embedded text is then divided into smaller, manageable chunks.\n",
        "3. **Query Expansion/Re-query**: User queries are expanded or re-queried to improve retrieval effectiveness.\n",
        "4. **Reranking**: Retrieved documents are reranked to prioritize the most relevant information.\n",
        "5. **Generation**: Finally, a language model generates answers based on the reranked, retrieved documents.\n",
        "\n",
        "## Stack\n",
        "The project leverages the following technologies:\n",
        "- **Langchain**: For orchestrating the RAG pipeline components.\n",
        "- **ChromaDB**: As the vector database for storing and retrieving embedded document chunks.\n",
        "- **Hugging Face (HF)**: For various NLP tasks, including models for transcription, tokenization, and potentially embeddings and text generation.\n",
        "- **Cohere**: Utilized for its powerful reranking capabilities to improve the relevance of retrieved documents.\n",
        "- **OpenAI**: For language model capabilities, potentially for summarization and query rewriting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG with Shorter Videos"
      ],
      "metadata": {
        "id": "quNxvrUOzxIq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bff396ff"
      },
      "source": [
        "pip install langchain_text_splitters langchain_community faiss-cpu langchain-huggingface cohere langchain-openai chromadb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "xrlXv4_ZJs2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the transcription of the all the audio files"
      ],
      "metadata": {
        "id": "LflVuOS1t0W7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "abdb853c"
      },
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import librosa\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "whisper_model = pipeline(\n",
        "  \"automatic-speech-recognition\",\n",
        "  model=\"openai/whisper-small\",\n",
        "  chunk_length_s=30,\n",
        "  device=device,\n",
        ")\n",
        "\n",
        "rl_audio_1 = \"YouTube_Agent-Reinforcement-Fine-Tuning-Will-Han_Media.mp3\"\n",
        "rl_audio_2 = \"YouTube_Efficient-Reinforcement-Learning-Rhythm_Media.mp3\"\n",
        "rl_audio_3 = \"YouTube_RL-Environments-at-Scale-Will-Brown-Prim_Media.mp3\"\n",
        "combined_audio = [rl_audio_1, rl_audio_2, rl_audio_3]\n",
        "transcription_all = []\n",
        "for audio in combined_audio:\n",
        "  raw_audio, sampling_rate = librosa.load(audio, sr=16000)\n",
        "  transcription = whisper_model(raw_audio.copy(), batch_size=8)[\"text\"]\n",
        "  transcription_all.append(transcription)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Making sure we got all the results and things look fine\n",
        "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "for idx, transcript in enumerate(transcription_all):\n",
        "  print(f'Transcript for source {idx + 1}: ', transcript[:300])\n",
        "  token_count = len(qwen_tokenizer.encode(transcript))\n",
        "  print(f\"The token count of the 'transcript' is: {token_count} tokens\")"
      ],
      "metadata": {
        "id": "7Z0F3URcvjNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To free up RAM space\n",
        "del whisper_model"
      ],
      "metadata": {
        "id": "GmVzfcCeIMmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "qwen_model = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-1.5B-Instruct\", max_new_tokens=1500, device=device)\n",
        "\n",
        "notes = []\n",
        "for i, transcript in tqdm(enumerate(transcription_all)):\n",
        "  current_audio = combined_audio[i]\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert at converting transcriptions into detailed analysis that can be used to learn the content of the transcription. Analyse the transcription and create a well-structured (according to current educational best practices) analysis about all the key points discussed. Start with an executive summary in a couple of sentence, not other special intro needed.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Discuss all the key points in detail of the provided transcript {transcript}. Make sure to structure your answer to facilitate learning from it, hence following the educational best practices. Avoid using bullet points, use full sentences and make it very detailed\"},\n",
        "]\n",
        "  note_content = qwen_model(messages)[0]['generated_text'][-1]['content']\n",
        "  notes.append({\n",
        "      \"summary\": note_content,\n",
        "      \"metadata\": {\"source\": current_audio}\n",
        "  })\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2rMxoTbflaXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notes[0]['summary']"
      ],
      "metadata": {
        "id": "2f8UAg7_yaie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b2d35e5"
      },
      "source": [
        "# Let's check the token length of each new summary note\n",
        "for idx, summary in enumerate(notes):\n",
        "  token_count = len(qwen_tokenizer.encode(summary['summary']))\n",
        "  print(f\"The token count of the Summary #{idx+1} is: {token_count} tokens\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del qwen_model"
      ],
      "metadata": {
        "id": "XJwrlL5mIbEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\n",
        "print(\n",
        "    f\"Model's maximum sequence length: {SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').max_seq_length}\"\n",
        ")"
      ],
      "metadata": {
        "id": "mvA8vRSVYgbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document as LangchainDocument\n",
        "\n",
        "YT_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=note[\"summary\"], metadata=note[\"metadata\"])\n",
        "    for note in tqdm(notes)\n",
        "]\n"
      ],
      "metadata": {
        "id": "NQPhi8nnYjMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from typing import Optional, List\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=False,\n",
        "        strip_whitespace=True,\n",
        "    )\n",
        "\n",
        "    notes_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        notes_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    return notes_processed\n",
        "\n",
        "\n",
        "notes_processed = split_documents(\n",
        "    256,  # We choose a chunk size adapted to our model\n",
        "    YT_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")"
      ],
      "metadata": {
        "id": "zfUfQE2HYIEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "lengths = [len(tokenizer.encode(note.page_content)) for note in tqdm(notes_processed)]\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l2ppld8i0ftP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    notes_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")"
      ],
      "metadata": {
        "id": "X953VviQYzaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_QUERY = \"What are the best practices of training an RL model or Agent?\""
      ],
      "metadata": {
        "id": "R5sBga7c75nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = embedding_model.embed_query(USER_QUERY)\n",
        "\n",
        "print(f\"\\nStarting retrieval for {USER_QUERY=}...\")\n",
        "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=USER_QUERY, k=6)\n",
        "print(\n",
        "    \"\\n==================================Top document==================================\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)\n",
        "print(\"==================================Metadata==================================\")\n",
        "print(retrieved_docs[0].metadata)"
      ],
      "metadata": {
        "id": "KDKfIXqC4iHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "retriever_model_name = \"LiquidAI/LFM2.5-1.2B-Instruct\"\n",
        "\n",
        "lqd_model = AutoModelForCausalLM.from_pretrained(\n",
        "    retriever_model_name\n",
        ").to(device)\n",
        "lqd_tokenizer = AutoTokenizer.from_pretrained(retriever_model_name)\n",
        "\n",
        "RETRIEVER_LLM = pipeline(\n",
        "    model=lqd_model,\n",
        "    tokenizer=lqd_tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=800,\n",
        ")"
      ],
      "metadata": {
        "id": "ecxZWNY-Y0AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Your responses should be relevant to the question and at the end, ask the user for any follow up question related to the response.\n",
        "If the answer cannot be deduced from the context, state that to the user.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "YT_RAG_PROMPT_TEMPLATE = lqd_tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "print(YT_RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "KkDVeF9CyvQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere as Cohere\n",
        "from google.colab import userdata\n",
        "COHERE_API_KEY = 'YOUR_COHERE_API_KEY'\n",
        "\n",
        "co = Cohere.ClientV2(api_key=COHERE_API_KEY)\n",
        "\n",
        "results = co.rerank(\n",
        "    model=\"rerank-v3.5\", query=USER_QUERY, documents=[doc.page_content for doc in retrieved_docs], top_n=3\n",
        ")\n",
        "\n",
        "# Display the reranking results\n",
        "for idx, result in enumerate(results.results):\n",
        "    print(f\"Rank: {idx+1}\")\n",
        "    print(f\"Score: {result.relevance_score}\")\n",
        "    print(f\"Document: {retrieved_docs[result.index]}\\n\")\n",
        "reranked_documents = [\n",
        "    retrieved_docs[result.index] for result in results.results\n",
        "]"
      ],
      "metadata": {
        "id": "hGy2TOV561kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.results"
      ],
      "metadata": {
        "id": "eiEcHRee6_rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker = co,\n",
        "    num_retrieved_docs: int = 6,\n",
        "    num_docs_final: int = 4,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
        "\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        reranked_relevant_docs = reranker.rerank(model=\"rerank-v3.5\", query=question, documents=relevant_docs, top_n=num_docs_final)\n",
        "        relevant_docs = [relevant_docs[doc.index] for doc in reranked_relevant_docs.results]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = relevant_docs\n",
        "\n",
        "    final_prompt = YT_RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ],
      "metadata": {
        "id": "0GsSqatz0isU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_answer, relevant_docs = answer_with_rag(USER_QUERY, RETRIEVER_LLM, KNOWLEDGE_VECTOR_DATABASE)\n",
        "print(f'Returned answer: ', rag_answer)\n",
        "print(f'Relevant docs for the returned answer: ', relevant_docs)"
      ],
      "metadata": {
        "id": "OW46B_nJDXPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Longer Video Input"
      ],
      "metadata": {
        "id": "bqKVGVG-zsZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pydantic_settings import BaseSettings\n",
        "from pydantic import BaseModel\n",
        "from google.colab import userdata\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class Settings(BaseSettings):\n",
        "  # Audio path\n",
        "  AUDIO_PATH: str = 'Building-Agents-with-Model-Context-Protocol.mp3'\n",
        "  AUDIO_SOURCE: str = 'Building-Agents-with-Model-Context-Protocol'\n",
        "  # Transcribe\n",
        "  AUDIO_MODEL: str = 'openai/whisper-large-v3-turbo'\n",
        "  # Generate text\n",
        "  TEXT_GENERATION: str = 'HuggingFaceTB/SmolLM3-3B'\n",
        "  OPENAI_KEY: str = 'YOUR_OPENAI_API_KEY'\n",
        "  HF_TOKEN: str = 'YOUR_HF_WRITE_ACCESS'\n",
        "  COHERE_API_KEY: str = 'YOUR_COHERE_API_KEY'\n",
        "  # RAG\n",
        "  TEXT_EMBEDDING_MODEL_ID: str = 'ibm-granite/granite-embedding-small-english-r2'\n",
        "  RERANKING_CROSS_ENCODER_MODEL_ID: str = 'cross-encoder/ms-marco-MiniLM-L6-v2'\n",
        "  RAG_MODEL_DEVICE: str = \"cpu\"\n",
        "\n",
        "settings = Settings()"
      ],
      "metadata": {
        "id": "7bCT8eQD-wpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "class QueryRewritingTemplate(BaseModel):\n",
        "    prompt: str = \"\"\"You are an expert at paraphrasing user queries and breaking them down into smaller subqueries.\n",
        "    Your task is to first paraphrase/make it more concrete, clear and then break this down {break_down_to_n}\n",
        "    different sub-queries of the given user question to retrieve relevant documents from a vector\n",
        "    database. By paraphrasing the user question, your aim is to add further more concrete details\n",
        "    that could help the user to get the full or better picture. Then, by breaking down such paraphrased query,\n",
        "    your goal is to find and retrieve the little nuances of the query. Provide these sub-queries seperated by '{separator}'.\n",
        "    Original question: {question}\n",
        "    \"\"\"\n",
        "\n",
        "    @property\n",
        "    def separator(self) -> str:\n",
        "        return \"#next-subquery#\"\n",
        "\n",
        "    def create_template(self, break_down_to_n: int) -> ChatPromptTemplate:\n",
        "        base_prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\"human\", self.prompt)\n",
        "            ]\n",
        "        )\n",
        "        return base_prompt.partial(\n",
        "            separator=self.separator,\n",
        "            break_down_to_n=break_down_to_n,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "qzEmtWKt-lvr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "def transcribe(audio_path:str, model_name:str, device:str, chunk_length=30):\n",
        "  speech_rec_model = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model_name,\n",
        "    chunk_length_s=chunk_length,\n",
        "    stride_length_s=chunk_length / 3,\n",
        "    device=device,\n",
        "  )\n",
        "  raw_audio, sampling_rate = librosa.load(audio_path, sr=16000)\n",
        "  transcription = speech_rec_model(raw_audio.copy(), batch_size=8)[\"text\"]\n",
        "\n",
        "  del speech_rec_model\n",
        "\n",
        "  return transcription\n",
        "\n",
        "transcription = transcribe(settings.AUDIO_PATH, settings.AUDIO_MODEL, device)"
      ],
      "metadata": {
        "id": "RBoG0zHVzvFv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\n",
        "print(\n",
        "    f\"Model's maximum sequence length: {SentenceTransformer(settings.TEXT_EMBEDDING_MODEL_ID).max_seq_length}\"\n",
        ")"
      ],
      "metadata": {
        "id": "nG-b7LIKQ_t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from typing import Optional, List, Tuple\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    transcription: str,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "    )\n",
        "\n",
        "    knowledge_base = LangchainDocument(page_content=transcription, metadata={'summary': settings.AUDIO_SOURCE})\n",
        "\n",
        "    transcript_processed = text_splitter.split_documents([knowledge_base])\n",
        "\n",
        "    return transcript_processed\n",
        "\n",
        "# Using a not too big token number to retain some granularity\n",
        "chunked_transcript = split_documents(2500, transcription)"
      ],
      "metadata": {
        "id": "UM8a1iIMjJev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_summ_metadata(chunked_transcript: List[LangchainDocument]):\n",
        "  sum_metadata = ChatOpenAI(model=\"gpt-5-nano\", api_key=settings.OPENAI_KEY)\n",
        "\n",
        "  for transcript in chunked_transcript:\n",
        "    # Come up with 1 sentence summary for each chunk\n",
        "    summary_prompt = f\"Write a one sentence summary of what the key topic/points of the passage is. This will be used for filtering purposes to get what the content is about more quickly. It should strictly be just one sentence. Chunk: {transcript.page_content}\"\n",
        "    response_summary = sum_metadata.invoke(summary_prompt)\n",
        "\n",
        "    # Modify the 'summary' metadata with the response of the LLM\n",
        "    transcript.metadata['summary'] = response_summary.content\n",
        "\n",
        "  return chunked_transcript\n",
        "\n",
        "chunked_transcript = add_summ_metadata(chunked_transcript)"
      ],
      "metadata": {
        "id": "2CeP8RoMetG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_transcript[0].metadata"
      ],
      "metadata": {
        "id": "y6Waqpyji3l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Emebdding function for HF\n",
        "import requests\n",
        "import numpy as np\n",
        "from chromadb.api.types import EmbeddingFunction\n",
        "\n",
        "class HuggingFaceRouterEmbeddingFunction(EmbeddingFunction):\n",
        "\n",
        "    def __init__(self, api_key, model_name):\n",
        "        self.api_key = api_key\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.url = (\n",
        "            f\"https://router.huggingface.co/hf-inference/models/\"\n",
        "            f\"{model_name}/pipeline/feature-extraction\"\n",
        "        )\n",
        "\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, texts):\n",
        "        payload = {\"inputs\": texts}\n",
        "\n",
        "        r = requests.post(self.url, headers=self.headers, json=payload)\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            raise RuntimeError(\n",
        "                f\"HuggingFace API error {r.status_code}: {r.text}\"\n",
        "            )\n",
        "\n",
        "        data = r.json()\n",
        "\n",
        "        # Return as float32 numpy arrays (what Chroma expects)\n",
        "        return [np.array(vec, dtype=np.float32) for vec in data]"
      ],
      "metadata": {
        "id": "FmQaArUyudUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document as LangchainDocument\n",
        "import chromadb\n",
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_transcripts_db\")\n",
        "# this is where the indexed embeddings, transcripts and metadata will be stored\n",
        "transcript_collection = chroma_client.create_collection(\n",
        "    name=\"yt_long_transcript\",\n",
        "    embedding_function=HuggingFaceRouterEmbeddingFunction(\n",
        "      api_key=settings.HF_TOKEN,\n",
        "      model_name=settings.TEXT_EMBEDDING_MODEL_ID\n",
        "    ),\n",
        "    configuration={\n",
        "        \"hnsw\": {\n",
        "            \"space\": \"cosine\",\n",
        "            \"ef_construction\": 250 # size of the candidate list used to select neighbors during index creation: high number means better accuracy, but more compute cost\n",
        "        }\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "Yr0moBoHfjc2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_collection.add(\n",
        "    ids=[f\"mcp_transcript_{i}\" for i in range(len(chunked_transcript))],\n",
        "    documents=[\n",
        "      transcript.page_content\n",
        "      for transcript in chunked_transcript\n",
        "    ],\n",
        "    metadatas=[{'summary': summary.metadata['summary']} for summary in chunked_transcript]\n",
        ")"
      ],
      "metadata": {
        "id": "WVCQv1h_onea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_collection.get(ids=['mcp_transcript_0'])"
      ],
      "metadata": {
        "id": "ky5qgP0V6kf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_database(query_text, n_results=10):\n",
        "    results = transcript_collection.query(query_texts=query_text, n_results=n_results, include=['documents', 'metadatas'])\n",
        "    return results"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oHSF5Mw2xlBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the model and creating the prompt"
      ],
      "metadata": {
        "id": "IoqH54T0GBS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(settings.TEXT_GENERATION)\n",
        "model = AutoModelForCausalLM.from_pretrained(settings.TEXT_GENERATION,\n",
        "                                             device_map=device,\n",
        "                                            torch_dtype=torch.bfloat16)\n"
      ],
      "metadata": {
        "id": "MSTIz1nLDk0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryRewriting():\n",
        "    def generate(self, query: str, break_down_to_n: int) -> str:\n",
        "        assert break_down_to_n > 0, f\"'break_down_to_n' should be greater than 0. Got {break_down_to_n}.\"\n",
        "\n",
        "        query_rewriting_template = QueryRewritingTemplate()\n",
        "        prompt = query_rewriting_template.create_template(break_down_to_n - 1)\n",
        "        model = ChatOpenAI(model='gpt-5-mini', api_key=settings.OPENAI_KEY, temperature=0.2)\n",
        "\n",
        "        chain = prompt | model\n",
        "\n",
        "        response = chain.invoke({\"question\": query})\n",
        "        sub_queries = response.content\n",
        "\n",
        "        queries_content = sub_queries.strip().split(query_rewriting_template.separator)\n",
        "\n",
        "        return queries_content"
      ],
      "metadata": {
        "id": "rt4z5WcC0fPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_llm_prompt = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the questions.\n",
        "Your responses should be relevant to the question and at the end, ask the user for any follow up question related to the response.\n",
        "If the answer cannot be deduced from the context, state that to the user.\n",
        "/no_think\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "YT_LONG_RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    final_llm_prompt, tokenize=False, add_generation_prompt=True\n",
        ")"
      ],
      "metadata": {
        "id": "GWtlnS437r90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(YT_LONG_RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "FAtouba48EOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "COHERE_API_KEY = 'YOUR_COHERE_API_KEY'\n",
        "\n",
        "def answer_from_transcript(user_query: str):\n",
        "  co = cohere.ClientV2(api_key=settings.COHERE_API_KEY)\n",
        "  query_rewriting = QueryRewriting()\n",
        "  rewrote_query = query_rewriting.generate(user_query, break_down_to_n=5)\n",
        "  contexts = []\n",
        "  for sub_query in rewrote_query:\n",
        "\n",
        "    db_finding = query_database(sub_query, n_results=7)\n",
        "\n",
        "    # Flatten the list of lists into a single list of strings for Cohere reranking\n",
        "    relevant_docs_flat = [item for sublist in db_finding['documents'] for item in sublist]\n",
        "\n",
        "    rerank_results = co.rerank(\n",
        "      model=\"rerank-v3.5\", query=user_query, documents=relevant_docs_flat, top_n=3\n",
        "    )\n",
        "    reranked_documents = [\n",
        "      relevant_docs_flat[result.index] for result in rerank_results.results\n",
        "    ]\n",
        "    contexts.append(reranked_documents)\n",
        "\n",
        "  final_prompt = YT_LONG_RAG_PROMPT_TEMPLATE.format(question=user_query, context=contexts)\n",
        "\n",
        "  input_ids = tokenizer(final_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "  outputs = model.generate(input_ids, max_new_tokens=3000)\n",
        "  return tokenizer.decode(outputs[0][input_ids.shape[-1]:])"
      ],
      "metadata": {
        "id": "0CmsF7FVGPna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_QUERY = \"What are the core concepts of MCP?\"\n",
        "response_rag = answer_from_transcript(USER_QUERY)"
      ],
      "metadata": {
        "id": "PPAlfcls_yYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_rag"
      ],
      "metadata": {
        "id": "Lwbb5GQELmBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}